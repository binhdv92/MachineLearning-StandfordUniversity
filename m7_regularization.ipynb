{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d8cafa",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "Machine Learning models need to generalize well to new examples that the model has not seen in practice. In this module, we introduce regularization, which helps prevent models from overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff37d6c",
   "metadata": {},
   "source": [
    "## The Problem of Overfitting\n",
    "The problem of overfitting and underfitting revolves around a model's ability to capture the true underlying of data versus its noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80316f2",
   "metadata": {},
   "source": [
    "### Three States of Modeling\n",
    "- **Underfitting (High Bias)**:\n",
    "    - **Cause**: The model is too simple (e.g., a linear function for non-linear data) or has a few features.\n",
    "    - **Effect**: It fails to capture the essential structure of the data, resulting in poor performance on both training and new data.\n",
    "\n",
    "- **Balanced Fit**:\n",
    "    - **Cause**: Adding an appropriate number of features (e.g., quadratic terms) to match the data's complexity.\n",
    "    - **Effect**: The model generalizes well to new, unseen data.\n",
    "\n",
    "- **Overfitting (High Variance)**:\n",
    "    - **Cause**: The model is overly complex (e.g., high-order polynomials)z.\n",
    "    - **Effect**: It fits the training data perfectly (passing through every point) but captures random noise rather than the trend, leading to poor predictions for new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64d78a",
   "metadata": {},
   "source": [
    "### How to Address Overfitting\n",
    "When a model is too complex for the training data, you can use two main strategies below to solve:\n",
    "- **Feature Reduction**: Manually or algorithmically remove less important features to simplify the hypothesis.\n",
    "- **Regularization**: Keep all features but reduce the magnitude (weight) of the parameters $\\theta_i$. This is particularly effective when you have many features that each contribute a small amount of information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cace26b",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "Let's explore how `Regularization` modifies the `Cost Function` to solve overfitting by penalizing large parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df1178",
   "metadata": {},
   "source": [
    "### Core Concept: Penalizing Complexity\n",
    "**The Strategy**: \n",
    "To prevent **overfitting**, we modify the `Cost Function` of the model. We don't mean to punish the model for being inccurate; but we punish it for having large parameter values ($\\theta$).\n",
    "\n",
    "**The Logic**:\n",
    "- **Large $\\theta$ values** create \"aggressive\" curves that over-react to noise in the data.\n",
    "- **Small $\\theta$ values** force those same curves to become flatter and smoother.\n",
    "\n",
    "**The Benefit**: \n",
    "By making the parameters smaller, the model simplifies its own hypothesis. It retains all available features but \"mutes\" their intensity, resulting in a curve that ignores random fuctuations and focuses on the true underlying trend. This allows the models to **generalize** (performing accurately on new data rather than just memorizing the training set).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ae5e4",
   "metadata": {},
   "source": [
    "### Regularized Cost Function\n",
    "To apply this to all features simultaneously, we add a `regularization term` to the cost function:\n",
    "$$\n",
    "J(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2+\\lambda\\sum_{j=1}^{n}\\theta_j^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **$\\lambda$ (Lambda)**: The regularization parameter. It controls the trade-off beetween fitting the training data and keeping the parameters small.\n",
    "- **The Penalty $(\\sum\\theta_j^2)$**: By convention, we regularize parameters from $j=1$ to $n$ (excluding the bias term $\\theta_0$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d35260",
   "metadata": {},
   "source": [
    "\n",
    "### Role of $\\lambda$ (Lambda)\n",
    "The **Regularization Parameter ($\\lambda$)** acts as a control knob that balances two goals: \n",
    "- Fitting the training data well.\n",
    "- Keeping the model simple.\n",
    "\n",
    "| $\\lambda$ value | Effect on Model | Risk |\n",
    "|---|---|---|\n",
    "| **Too Large** | Penalizes parameters too heavily; many $\\theta \\approx 0$ | **Underfitting** (High Bias) |\n",
    "| **Optimal** | Smoothes the curve while maintaining the trend | **Good Generalization** |\n",
    "| **Too Small / Zero** | The penalty term disappears; the model remains complex | **Overfitting** (High Variance) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e6154",
   "metadata": {},
   "source": [
    "## Regularized Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a950307",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "In the update rule, we separate $\\theta_0$ from the rest. For all other parameters, we add the regularization term $\\frac{\\lambda}{m}\\theta_j$:\n",
    "\n",
    "**Recall**: standard Gradient Descent Rules:\n",
    "$$\n",
    "\\theta_j:=\n",
    "\\theta_j-\\alpha\\begin{bmatrix}\n",
    "\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\n",
    "\\end{bmatrix}\\\\\n",
    "=\\theta_j-\\alpha\\begin{bmatrix}\n",
    "\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)}).x_j^{(i)}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccdca46",
   "metadata": {},
   "source": [
    "**The Update Rule**:\n",
    "$$\n",
    "\\theta_0:=\\theta_0-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)}).x_0^{(i)}\\\\\n",
    "\\theta_j:=\\theta_j-\\alpha\\begin{bmatrix}\n",
    "\\frac{\\partial}{\\partial\\theta_j}J^{(regularized)}(\\theta)\n",
    "\\end{bmatrix}=\\theta_j-\\alpha\\begin{bmatrix}(\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)}).x_0^{(i)})+\\frac{\\lambda}{m}\\theta_j\\end{bmatrix},\\ \n",
    "j\\in\\{1,2,...,n\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc194b3",
   "metadata": {},
   "source": [
    "Equivalent form:\n",
    "$$\n",
    "\\theta_0:=\\theta_0-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)}).x_0^{(i)}\\\\\n",
    "\\theta_j:=\\theta_j(1-\\alpha\\frac{\\lambda}{m}) - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)}).x_j^{(i)},\\ j\\in\\{1,2,\\cdots,n\\}\n",
    "$$\n",
    "\n",
    "- The term $(1-\\alpha\\frac{\\lambda}{m})$ is always be less than 1. This term reduces the values of $\\theta_j$ in very interation. Notice that the second term is now exactly the same as it was before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf6975",
   "metadata": {},
   "source": [
    "### Normal Equation\n",
    "**Regularization** can also be applied to the non-intertive normal equation. We add a matrix $L$ to the calculation to handle the penalty:\n",
    "$$\n",
    "X_{\\begin{bmatrix}m\\times (n+1)\\end{bmatrix}}=\n",
    "\\begin{bmatrix}\n",
    "    (x^{(1)^T})\\\\\n",
    "    \\vdots\\\\\n",
    "    (x^{(m)})^T\n",
    "\\end{bmatrix},  \n",
    "y_{[m\\times1]}=\n",
    "\\begin{bmatrix}\n",
    "    y^{(1)}\\\\\n",
    "    \\vdots\\\\\n",
    "    y^{(m)}\n",
    "\\end{bmatrix}\\\\\n",
    "\\theta_{[(n+1)\\times1]}=(X^TX+\\lambda\\cdot L)^{-1}X^Ty\n",
    "$$\n",
    "- The **$L$ Matrix**: An $(n+1)\\times (n+1)$ identity-like matrix, but with a **0** (zero) in the top-left corner (to ensure $\\theta_0$ is not penalized):\n",
    "$$\n",
    "L_{[(n+1)\\times(n+1)]}=\\begin{bmatrix}\n",
    "0       & 0         & \\cdots & 0\\\\\n",
    "0       & 1         & \\cdots & 0\\\\\n",
    "\\vdots  & \\vdots    & \\ddots & \\vdots\\\\\n",
    "0       & 0         & \\cdots & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d36e8",
   "metadata": {},
   "source": [
    "\n",
    "### Key Benefits\n",
    "- **Solves Non-Invertibility $(m\\le n)$**: If the number of features $n$ is greater than the number of examples $m$, the original $X^TX$ is non-invertible (singular). Adding $\\lambda\\cdot L$ makes the matrix **$[X^TX+\\lambda\\cdot L]$** invertible, ensuring a mathematical solution exists.\n",
    "- **Prevents Overfitting**: By discouraging large weights, the model becomes less sensitive to noise in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af122101",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f541ed6",
   "metadata": {},
   "source": [
    "---\n",
    "# The end!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
